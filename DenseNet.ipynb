{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import resnet_v2\n",
    "import tensorflow as tf\n",
    "from training import get_labels_from_annotation_batch\n",
    "slim = tf.contrib.slim\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "batch_images = tf.reshape(x, [-1, 28,28,1])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_probability\")\n",
    "is_training = tf.placeholder(tf.bool, name=\"batch_norm_switch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=12\n",
    "theta=1.0 # compression_factor\n",
    "number_of_classes = 10\n",
    "batch_norm_decay = 0.997\n",
    "batch_norm_epsilon = 1e-5,\n",
    "batch_norm_scale = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@slim.add_arg_scope\n",
    "def block_unit(net, k, id, keep_prob=1.0):\n",
    "    # block_unit implements a bottleneck layer \n",
    "    with tf.variable_scope(\"block_unit_\" + str(id)):\n",
    "        \n",
    "        # each 1×1 convolution produce 4k feature-maps\n",
    "        net = slim.conv2d(net, 4*k, [1,1], scope=\"conv1\", activation_fn=None, normalizer_fn=None)\n",
    "        net = slim.dropout(net, keep_prob, scope='dropout1')\n",
    "        net = slim.batch_norm(net, activation_fn=tf.nn.relu)\n",
    "\n",
    "        net = slim.conv2d(net, k, [3,3], scope=\"conv3x3\", activation_fn=None, normalizer_fn=None)\n",
    "        net = slim.dropout(net, keep_prob, scope='dropout2')\n",
    "        net = slim.batch_norm(net, activation_fn=tf.nn.relu)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@slim.add_arg_scope\n",
    "def add_transition_layer(net, id):\n",
    "    # We use 1×1 convolution followed by 2×2 average pooling as transition layers between two contiguous dense blocks\n",
    "    with tf.variable_scope(\"transition_layer_\" + str(id)):  \n",
    "        current_depth = slim.utils.last_dimension(net.get_shape(), min_rank=4)\n",
    "\n",
    "        net = slim.batch_norm(net, activation_fn=tf.nn.relu)\n",
    "        net = slim.conv2d(net, theta*current_depth, [1,1], scope='conv1x1', \n",
    "                          activation_fn=None, normalizer_fn=None)\n",
    "        net = slim.dropout(net, keep_prob, scope='dropout')\n",
    "        net = slim.avg_pool2d(net, [2,2], scope='avg_pool', stride=2)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_block_1/unit1_2_3_4:0\", shape=(?, 28, 28, 48), dtype=float32)\n",
      "Tensor(\"transition_layer_1/avg_pool/AvgPool:0\", shape=(?, 14, 14, 48), dtype=float32)\n",
      "Tensor(\"dense_block_2/unit1_2_3_4:0\", shape=(?, 14, 14, 48), dtype=float32)\n",
      "Tensor(\"transition_layer_2/avg_pool/AvgPool:0\", shape=(?, 7, 7, 48), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_norm_params = {\n",
    "  'decay': batch_norm_decay,\n",
    "  'epsilon': batch_norm_epsilon,\n",
    "  'scale': batch_norm_scale,\n",
    "  'is_training': is_training,\n",
    "  'updates_collections': tf.GraphKeys.UPDATE_OPS,\n",
    "}\n",
    "\n",
    "# batch_images shape (?, 224,224,3)\n",
    "with slim.arg_scope([slim.conv2d], padding='SAME',\n",
    "                    weights_initializer=slim.variance_scaling_initializer(),\n",
    "                    activation_fn=tf.nn.relu,\n",
    "                    stride=1,\n",
    "                    normalizer_fn=slim.batch_norm,\n",
    "                    normalizer_params=batch_norm_params,\n",
    "                    weights_regularizer=slim.l2_regularizer(0.0001)):\n",
    "\n",
    "    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "\n",
    "        # We refer to layers between blocks as transition layers, which do convolution and pooling\n",
    "        # net = slim.conv2d(batch_images, 2*k, [7,7], scope='conv1')\n",
    "        # net = slim.max_pool2d(net, [3,3], scope='pool1') # (?,56,56,2*k)\n",
    "\n",
    "        net = slim.conv2d(batch_images, 2*k, [7,7], scope='conv1')\n",
    "\n",
    "        with tf.variable_scope(\"dense_block_1\"):\n",
    "            unit1 = block_unit(net, k, 1)\n",
    "\n",
    "            unit2 = block_unit(unit1, k, 2)\n",
    "            unit1_2 = tf.concat((unit1, unit2), axis=3, name=\"unit1_2\")\n",
    "\n",
    "            unit3 = block_unit(unit1_2, k, 3)\n",
    "            unit1_2_3 = tf.concat((unit1_2, unit3), axis=3, name=\"unit1_2_3\")\n",
    "\n",
    "            unit4 = block_unit(unit1_2_3, k, 4)\n",
    "            block_1 = tf.concat((unit1_2_3, unit4), axis=3, name=\"unit1_2_3_4\")\n",
    "        \n",
    "        net = add_transition_layer(block_1, 1)\n",
    "        \n",
    "        with tf.variable_scope(\"dense_block_2\"):\n",
    "            unit1 = block_unit(net, k, 1)\n",
    "\n",
    "            unit2 = block_unit(unit1, k, 2)\n",
    "            unit1_2 = tf.concat((unit1, unit2), axis=3, name=\"unit1_2\")\n",
    "\n",
    "            unit3 = block_unit(unit1_2, k, 3)\n",
    "            unit1_2_3 = tf.concat((unit1_2, unit3), axis=3, name=\"unit1_2_3\")\n",
    "\n",
    "            unit4 = block_unit(unit1_2_3, k, 4)\n",
    "            block_2 = tf.concat((unit1_2_3, unit4), axis=3, name=\"unit1_2_3_4\")\n",
    "        \n",
    "        net = add_transition_layer(block_2, 2)\n",
    "        \n",
    "        with tf.variable_scope(\"dense_block_3\"):\n",
    "            unit1 = block_unit(net, k, 1)\n",
    "\n",
    "            unit2 = block_unit(unit1, k, 2)\n",
    "            unit1_2 = tf.concat((unit1, unit2), axis=3, name=\"unit1_2\")\n",
    "\n",
    "            unit3 = block_unit(unit1_2, k, 3)\n",
    "            unit1_2_3 = tf.concat((unit1_2, unit3), axis=3, name=\"unit1_2_3\")\n",
    "\n",
    "            unit4 = block_unit(unit1_2_3, k, 4)\n",
    "            block_3 = tf.concat((unit1_2_3, unit4), axis=3, name=\"unit1_2_3_4\")\n",
    "        \n",
    "        print(block_3)\n",
    "        net = slim.batch_norm(block_3, activation_fn=tf.nn.relu)\n",
    "        \n",
    "        # Flatten the feature map by global average pooling\n",
    "        global_avg_pooling = tf.reduce_mean(net, (1, 2))\n",
    "        logits = slim.fully_connected(global_avg_pooling, number_of_classes, scope='fully_connected', \n",
    "                                      activation_fn=None, normalizer_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 40\n",
    "initial_lr = 0.1\n",
    "second_lr = initial_lr/10\n",
    "third_lr = second_lr/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "boundaries = [1280, 1920]\n",
    "values = [initial_lr, second_lr, third_lr]\n",
    "learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "train_step = slim.learning.create_train_op(cross_entropy, optimizer, global_step=global_step)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "if update_ops:\n",
    "    updates = tf.group(*update_ops)\n",
    "    cross_entropy = control_flow_ops.with_dependencies([updates], cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.337 Training accuracy: 0.0625\n",
      "Step: 100 Loss: 0.138563 Training accuracy: 0.078125\n",
      "Step: 200 Loss: 0.171861 Training accuracy: 0.078125\n",
      "Step: 300 Loss: 0.0674317 Training accuracy: 0.109375\n",
      "Step: 400 Loss: 0.0968171 Training accuracy: 0.78125\n",
      "Step: 500 Loss: 0.117412 Training accuracy: 0.671875\n",
      "Step: 600 Loss: 0.276976 Training accuracy: 0.515625\n",
      "Step: 700 Loss: 0.232482 Training accuracy: 0.859375\n",
      "Step: 800 Loss: 0.103286 Training accuracy: 0.765625\n",
      "Step: 900 Loss: 0.159462 Training accuracy: 0.796875\n",
      "Step: 1000 Loss: 0.0170348 Training accuracy: 0.953125\n",
      "Step: 1100 Loss: 0.0238303 Training accuracy: 0.921875\n",
      "Step: 1200 Loss: 0.083677 Training accuracy: 0.875\n",
      "Step: 1300 Loss: 0.0357825 Training accuracy: 0.9375\n",
      "Step: 1400 Loss: 0.0044147 Training accuracy: 1.0\n",
      "Step: 1500 Loss: 0.00961189 Training accuracy: 0.984375\n",
      "Step: 1600 Loss: 0.00329354 Training accuracy: 1.0\n",
      "Step: 1700 Loss: 0.00616657 Training accuracy: 1.0\n",
      "Step: 1800 Loss: 0.0013633 Training accuracy: 1.0\n",
      "Step: 1900 Loss: 0.0183842 Training accuracy: 1.0\n",
      "test accuracy 0.9924\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(2000):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0, is_training: True})\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "              x: batch[0], y_: batch[1], keep_prob: 1.0, is_training: False})\n",
    "\n",
    "            print(\"Step:\", i, \"Loss:\", loss, \"Training accuracy:\", train_accuracy )\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0,  is_training: False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
